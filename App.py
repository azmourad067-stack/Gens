import streamlit as st
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup
import re
import json
from datetime import datetime
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# ML avanc√©
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor
from sklearn.linear_model import Ridge, ElasticNet
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import cross_val_score, KFold, train_test_split
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.neural_network import MLPRegressor
import xgboost as xgb
import warnings
warnings.filterwarnings('ignore')

st.set_page_config(
    page_title="üèá Analyseur Hippique IA Pro+",
    page_icon="üèá",
    layout="wide"
)

# Configuration avanc√©e des types de courses
RACE_CONFIGS = {
    "PLAT": {
        "description": "üèÉ Course de galop - Importance du poids et de la corde",
        "optimal_draws": [1, 2, 3, 4],
        "weight_coef": 0.25,
        "draw_coef": 0.15,
        "features_priority": ['odds_inv', 'music_recent_form', 'weight_advantage', 'optimal_draw']
    },
    "ATTELE_AUTOSTART": {
        "description": "üöó Trot attel√© autostart - Importance position d√©part",
        "optimal_draws": [4, 5, 6],
        "weight_coef": 0.05,
        "draw_coef": 0.25,
        "features_priority": ['odds_inv', 'optimal_draw', 'music_consistency', 'driver_skill']
    },
    "ATTELE_VOLTE": {
        "description": "üîÑ Trot attel√© volt√© - Importance r√©gularit√©",
        "optimal_draws": [],
        "weight_coef": 0.05,
        "draw_coef": 0.05,
        "features_priority": ['odds_inv', 'music_consistency', 'music_recent_form', 'driver_skill']
    },
    "OBSTACLE": {
        "description": "üèá Course d'obstacles - Exp√©rience et technique",
        "optimal_draws": [2, 3, 4, 5],
        "weight_coef": 0.20,
        "draw_coef": 0.10,
        "features_priority": ['music_win_rate', 'music_consistency', 'age_optimal', 'weight_advantage']
    }
}

class AdvancedRacingPredictor:
    """
    Syst√®me pr√©dictif avanc√© pour les courses hippiques
    Utilise l'apprentissage automatique pour pond√©rer automatiquement les facteurs
    """
    
    def __init__(self):
        self.models = {}
        self.scaler = RobustScaler()
        self.feature_importance = {}
        self.cv_results = {}
        self.is_trained = False
        
    def create_advanced_models(self, n_samples):
        """Cr√©e une ensemble de mod√®les avanc√©s adapt√©s √† la taille des donn√©es"""
        base_models = {
            'xgboost': xgb.XGBRegressor(
                n_estimators=100 if n_samples < 20 else 200,
                max_depth=4 if n_samples < 15 else 6,
                learning_rate=0.05,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=42
            ),
            'random_forest': RandomForestRegressor(
                n_estimators=100 if n_samples < 20 else 150,
                max_depth=6 if n_samples < 15 else 8,
                min_samples_split=5 if n_samples < 10 else 8,
                min_samples_leaf=2 if n_samples < 10 else 4,
                random_state=42,
                n_jobs=-1
            ),
            'gradient_boosting': GradientBoostingRegressor(
                n_estimators=80 if n_samples < 20 else 100,
                learning_rate=0.08,
                max_depth=4 if n_samples < 15 else 5,
                min_samples_split=8 if n_samples < 15 else 10,
                random_state=42
            )
        }
        
        # Ajout du r√©seau de neurones seulement si assez de donn√©es
        if n_samples >= 15:
            base_models['neural_network'] = MLPRegressor(
                hidden_layer_sizes=(32, 16) if n_samples < 30 else (64, 32, 16),
                activation='relu',
                learning_rate_init=0.001,
                max_iter=300,
                random_state=42,
                early_stopping=True,
                validation_fraction=0.2,  # Augment√© pour petites datasets
                n_iter_no_change=10
            )
        
        self.models = base_models
    
    def extract_comprehensive_features(self, music_str):
        """
        Extraction avanc√©e des performances pass√©es avec pond√©ration temporelle
        Les courses r√©centes ont plus de poids que les anciennes
        """
        if pd.isna(music_str) or music_str == '':
            return self._get_default_music_features()
        
        music = str(music_str)
        # Extraction des positions (supprime les lettres)
        positions = []
        for char in music:
            if char.isdigit():
                pos = int(char)
                if 1 <= pos <= 9:  # Positions valides en course
                    positions.append(pos)
        
        if not positions:
            return self._get_default_music_features()
        
        # Pond√©ration temporelle : les courses r√©centes comptent plus
        weights = np.linspace(1.0, 0.3, len(positions))  # D√©croissance lin√©aire
        weighted_positions = [p * w for p, w in zip(positions, weights)]
        
        total_weighted = sum(weights)
        weighted_avg = sum(weighted_positions) / total_weighted if total_weighted > 0 else 0
        
        # Calculs avec pond√©ration
        wins = sum(1 for p, w in zip(positions, weights) if p == 1)
        places = sum(w for p, w in zip(positions, weights) if p <= 3)
        
        total_races = len(positions)
        recent_races = positions[:min(3, len(positions))]
        
        return {
            'wins': wins,
            'places': places,
            'total_races': total_races,
            'win_rate': wins / total_races if total_races > 0 else 0,
            'place_rate': places / total_races if total_races > 0 else 0,
            'weighted_avg_position': weighted_avg,
            'recent_form': 1 / (np.mean(recent_races) + 0.1) if recent_races else 0,
            'consistency': 1 / (np.std(positions) + 0.5) if len(positions) > 1 else 0.5,
            'best_position': min(positions),
            'momentum': self._calculate_momentum(positions),
            'recovery_ability': self._calculate_recovery(positions)
        }
    
    def _get_default_music_features(self):
        """Retourne des features par d√©faut pour donn√©es manquantes"""
        return {
            'wins': 0, 'places': 0, 'total_races': 0,
            'win_rate': 0, 'place_rate': 0, 'weighted_avg_position': 8,
            'recent_form': 0, 'consistency': 0.5, 'best_position': 10,
            'momentum': 0, 'recovery_ability': 0
        }
    
    def _calculate_momentum(self, positions):
        """Calcule la dynamique r√©cente du cheval"""
        if len(positions) < 2:
            return 0
        recent = positions[:3]
        return sum(1/(p+0.1) for p in recent) / len(recent)
    
    def _calculate_recovery(self, positions):
        """Calcule la capacit√© √† rebondir apr√®s une mauvaise performance"""
        if len(positions) < 2:
            return 0
        recoveries = 0
        for i in range(1, len(positions)):
            if positions[i] < positions[i-1]:  # Am√©lioration
                recoveries += 1
        return recoveries / (len(positions) - 1)
    
    def prepare_advanced_features(self, df, race_type="PLAT"):
        """
        Pr√©paration compl√®te des features avec ing√©nierie avanc√©e
        """
        features = pd.DataFrame()
        config = RACE_CONFIGS[race_type]
        
        # === FEATURES DE BASE AVANC√âES ===
        features['odds_inv'] = 1 / (df['odds_numeric'] + 0.01)
        features['log_odds'] = np.log1p(df['odds_numeric'])
        features['odds_rank'] = df['odds_numeric'].rank(pct=True)
        
        # === FEATURES DE POSITION INTELLIGENTES ===
        features['draw'] = df['draw_numeric']
        features['draw_normalized'] = df['draw_numeric'] / max(df['draw_numeric'].max(), 1)
        features['optimal_draw'] = df['draw_numeric'].apply(
            lambda x: 1 if x in config['optimal_draws'] else 0
        )
        
        # Distance √† la position optimale
        if config['optimal_draws']:
            features['draw_penalty'] = df['draw_numeric'].apply(
                lambda x: min(abs(x - opt) for opt in config['optimal_draws']) / len(config['optimal_draws'])
            )
        else:
            features['draw_penalty'] = 0
        
        # === FEATURES DE POIDS CONTEXTUELLES ===
        features['weight'] = df['weight_kg']
        features['weight_advantage'] = (df['weight_kg'].max() - df['weight_kg']) * config['weight_coef']
        features['weight_rank'] = df['weight_kg'].rank(pct=True)
        
        # === FEATURES D√âMOGRAPHIQUES ===
        if '√Çge/Sexe' in df.columns:
            features['age'] = df['√Çge/Sexe'].str.extract('(\d+)').astype(float).fillna(4)
            features['is_female'] = df['√Çge/Sexe'].str.contains('F', na=False).astype(int)
            features['is_gelding'] = df['√Çge/Sexe'].str.contains('H', na=False).astype(int)
            features['age_optimal'] = features['age'].apply(lambda x: 1 if 3.5 <= x <= 6.5 else 0)
            features['age_experience'] = np.log1p(features['age'])
        else:
            features['age'] = 4.5
            features['is_female'] = 0
            features['is_gelding'] = 0
            features['age_optimal'] = 1
            features['age_experience'] = np.log1p(4.5)
        
        # === FEATURES DE PERFORMANCE D√âTAILL√âES ===
        if 'Musique' in df.columns:
            music_features = df['Musique'].apply(self.extract_comprehensive_features)
            for key in music_features.iloc[0].keys():
                features[f'music_{key}'] = [m[key] for m in music_features]
        else:
            default_features = self._get_default_music_features()
            for key in default_features.keys():
                features[f'music_{key}'] = default_features[key]
        
        # === FEATURES D'INTERACTION AVANC√âES ===
        features['odds_form_interaction'] = features['odds_inv'] * features['music_recent_form']
        features['weight_age_interaction'] = features['weight_advantage'] * features['age_optimal']
        features['draw_odds_interaction'] = features['optimal_draw'] * features['odds_inv']
        features['consistency_advantage'] = features['music_consistency'] * features['weight_advantage']
        
        # === FEATURES DE CONTEXTE DE COURSE ===
        features['field_size'] = len(df)
        features['competitiveness'] = 1 / (df['odds_numeric'].std() + 0.1)
        features['favorite_pressure'] = (df['odds_numeric'] == df['odds_numeric'].min()).astype(int)
        
        # === FEATURES STATISTIQUES RELATIVES ===
        features['win_rate_rank'] = features['music_win_rate'].rank(pct=True)
        features['form_rank'] = features['music_recent_form'].rank(pct=True)
        features['consistency_rank'] = features['music_consistency'].rank(pct=True)
        
        return features.fillna(0)
    
    def create_synthetic_labels(self, X, race_type="PLAT"):
        """
        Cr√©e des labels synth√©tiques r√©alistes bas√©s sur les features
        """
        config = RACE_CONFIGS[race_type]
        
        # Pond√©ration selon le type de course
        base_weights = {
            'odds_inv': 0.35,
            'music_recent_form': 0.20,
            'music_consistency': 0.15,
            'weight_advantage': config['weight_coef'],
            'optimal_draw': config['draw_coef'],
            'music_win_rate': 0.10,
            'age_optimal': 0.05
        }
        
        # Calcul du score synth√©tique
        y_synthetic = np.zeros(len(X))
        for feature, weight in base_weights.items():
            if feature in X.columns:
                # Normalisation de la feature
                feature_norm = (X[feature] - X[feature].min()) / (X[feature].max() - X[feature].min() + 1e-8)
                y_synthetic += feature_norm * weight
        
        # Ajout d'un bruit r√©aliste
        noise = np.random.normal(0, 0.03, len(X))
        y_synthetic += noise
        
        return np.clip(y_synthetic, 0, 1)
    
    def train_ensemble_model(self, X, y):
        """
        Entra√Ænement d'un ensemble de mod√®les adapt√© √† la taille des donn√©es
        """
        n_samples = len(X)
        
        # Ajustement du nombre de folds selon la taille des donn√©es
        if n_samples < 8:
            cv_folds = 3
        elif n_samples < 15:
            cv_folds = 4
        else:
            cv_folds = 5
            
        kf = KFold(n_splits=cv_folds, shuffle=True, random_state=42)
        
        # Normalisation des features
        X_scaled = self.scaler.fit_transform(X)
        
        # Validation crois√©e pour chaque mod√®le
        cv_scores = {}
        predictions = {}
        
        for name, model in self.models.items():
            try:
                # Pour les tr√®s petits datasets, on utilise une validation simple
                if n_samples < 6:
                    model.fit(X_scaled, y)
                    pred = model.predict(X_scaled)
                    r2 = r2_score(y, pred)
                    cv_scores[name] = {
                        'mean_r2': max(r2, 0),  # √âvite les scores n√©gatifs
                        'std_r2': 0.1,
                        'scores': [r2]
                    }
                else:
                    # Validation crois√©e standard
                    scores = cross_val_score(model, X_scaled, y, cv=kf, scoring='r2', n_jobs=-1)
                    cv_scores[name] = {
                        'mean_r2': max(scores.mean(), 0),
                        'std_r2': scores.std(),
                        'scores': scores
                    }
                    model.fit(X_scaled, y)
                    pred = model.predict(X_scaled)
                
                predictions[name] = pred
                
                # Importance des features (si disponible)
                if hasattr(model, 'feature_importances_'):
                    self.feature_importance[name] = dict(
                        sorted(zip(X.columns, model.feature_importances_), 
                              key=lambda x: x[1], reverse=True)[:10]
                    )
                    
            except Exception as e:
                st.warning(f"‚ö†Ô∏è Erreur avec le mod√®le {name}: {str(e)}")
                # Pr√©diction de fallback
                predictions[name] = np.full(len(X), y.mean())
                cv_scores[name] = {'mean_r2': 0.1, 'std_r2': 0.1, 'scores': [0.1]}
        
        return predictions, cv_scores
    
    def optimize_ensemble_weights(self, predictions, y_true):
        """
        Optimise les poids de l'ensemble pour maximiser la performance
        """
        # M√©thode simplifi√©e pour petites datasets
        if len(y_true) < 6:
            # Poids √©gaux pour petites datasets
            return np.ones(len(predictions)) / len(predictions)
        
        try:
            from scipy.optimize import minimize
            
            def objective(weights):
                combined = sum(w * pred for w, pred in zip(weights, predictions.values()))
                return -r2_score(y_true, combined)
            
            constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})
            bounds = [(0, 1) for _ in predictions]
            x0 = np.ones(len(predictions)) / len(predictions)
            
            result = minimize(objective, x0, method='SLSQP', bounds=bounds, constraints=constraints)
            return result.x
        except:
            return np.ones(len(predictions)) / len(predictions)
    
    def predict_with_confidence(self, X, race_type="PLAT"):
        """
        Pr√©diction avec estimation de la confiance adapt√©e √† la taille des donn√©es
        """
        n_samples = len(X)
        
        if n_samples < 3:
            st.warning("‚ö†Ô∏è Donn√©es insuffisantes pour une pr√©diction fiable")
            return np.zeros(n_samples), {}, np.zeros(n_samples)
        
        # Cr√©ation des mod√®les adapt√©s
        self.create_advanced_models(n_samples)
        
        # Pr√©paration des labels synth√©tiques
        y_synthetic = self.create_synthetic_labels(X, race_type)
        
        # Entra√Ænement de l'ensemble
        predictions, cv_scores = self.train_ensemble_model(X, y_synthetic)
        
        if not predictions:
            st.error("‚ùå Aucun mod√®le n'a pu √™tre entra√Æn√©")
            return np.zeros(n_samples), {}, np.zeros(n_samples)
        
        # Optimisation des poids de l'ensemble
        optimal_weights = self.optimize_ensemble_weights(predictions, y_synthetic)
        
        # Pr√©diction finale pond√©r√©e
        final_predictions = sum(
            weight * pred for weight, pred in zip(optimal_weights, predictions.values())
        )
        
        # Calcul de la confiance adaptatif
        confidence = self.calculate_adaptive_confidence(final_predictions, X, cv_scores, n_samples)
        
        self.is_trained = True
        self.cv_results = cv_scores
        
        return final_predictions, cv_scores, confidence
    
    def calculate_adaptive_confidence(self, predictions, X, cv_scores, n_samples):
        """
        Calcule un score de confiance adapt√© √† la taille des donn√©es
        """
        if n_samples < 3:
            return np.ones(n_samples) * 0.3
        
        # Facteur de base selon la taille des donn√©es
        size_factor = min(n_samples / 20, 1.0)  # Normalis√© par rapport √† 20 √©chantillons
        
        # 1. Variabilit√© des pr√©dictions
        pred_variance = np.var(predictions)
        confidence_variance = 1 / (1 + pred_variance * 10)
        
        # 2. Qualit√© des donn√©es
        data_quality = 1 - (X.isna().sum(axis=1) / len(X.columns))
        
        # 3. Performance des mod√®les
        avg_r2 = np.mean([scores['mean_r2'] for scores in cv_scores.values()])
        model_confidence = max(0, min(1, avg_r2 + 0.3))
        
        # Combinaison avec facteur de taille
        confidence = (
            confidence_variance * 0.3 +
            data_quality.values * 0.3 +
            model_confidence * 0.2 +
            size_factor * 0.2
        )
        
        return np.clip(confidence, 0.1, 0.95)

# =============================================================================
# FONCTIONS EXISTANTES
# =============================================================================

def safe_convert(value, convert_func, default=0):
    try:
        if pd.isna(value):
            return default
        cleaned = str(value).replace(',', '.').strip()
        return convert_func(cleaned)
    except:
        return default

def prepare_enhanced_data(df):
    """Pr√©paration des donn√©es avec gestion d'erreurs am√©lior√©e"""
    df = df.copy()
    
    # Conversion robuste des cotes
    df['odds_numeric'] = df['Cote'].apply(lambda x: safe_convert(x, float, 99.0))
    
    # Conversion des num√©ros de corde
    df['draw_numeric'] = df['Num√©ro de corde'].apply(lambda x: safe_convert(x, int, 1))
    
    # Extraction du poids
    def extract_weight_enhanced(poids_str):
        if pd.isna(poids_str):
            return 60.0
        try:
            matches = re.findall(r'\d+[.,]\d+|\d+', str(poids_str))
            if matches:
                return float(matches[0].replace(',', '.'))
        except:
            pass
        return 60.0
    
    df['weight_kg'] = df['Poids'].apply(extract_weight_enhanced)
    
    # Filtrage des donn√©es aberrantes
    df = df[(df['odds_numeric'] > 0) & (df['odds_numeric'] < 100)]
    df = df.reset_index(drop=True)
    
    return df

@st.cache_data(ttl=300)
def scrape_race_data(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        response = requests.get(url, headers=headers, timeout=10)
        
        if response.status_code != 200:
            return None, f"Erreur HTTP {response.status_code}"

        soup = BeautifulSoup(response.content, 'html.parser')
        horses_data = []
        
        table = soup.find('table')
        if not table:
            return None, "Aucun tableau trouv√©"
            
        rows = table.find_all('tr')[1:]
        
        for row in rows:
            cols = row.find_all(['td', 'th'])
            if len(cols) >= 4:
                horses_data.append({
                    "Num√©ro de corde": cols[0].get_text(strip=True),
                    "Nom": cols[1].get_text(strip=True),
                    "Cote": cols[-1].get_text(strip=True),
                    "Poids": cols[-2].get_text(strip=True) if len(cols) > 4 else "60.0",
                    "Musique": cols[2].get_text(strip=True) if len(cols) > 5 else "",
                    "√Çge/Sexe": cols[3].get_text(strip=True) if len(cols) > 6 else "",
                })

        if not horses_data:
            return None, "Aucune donn√©e extraite"
            
        return pd.DataFrame(horses_data), "Succ√®s"
        
    except Exception as e:
        return None, f"Erreur: {str(e)}"

def generate_sample_data(data_type="plat"):
    if data_type == "plat":
        return pd.DataFrame({
            'Nom': ['Thunder Bolt', 'Lightning Star', 'Storm King', 'Rain Dance', 'Wind Walker', 'Fire Dancer', 'Ocean Wave'],
            'Num√©ro de corde': ['1', '2', '3', '4', '5', '6', '7'],
            'Cote': ['3.2', '4.8', '7.5', '6.2', '9.1', '12.5', '15.0'],
            'Poids': ['56.5', '57.0', '58.5', '59.0', '57.5', '60.0', '61.5'],
            'Musique': ['1a2a3a1a', '2a1a4a3a', '3a3a1a2a', '1a4a2a1a', '4a2a5a3a', '5a3a6a4a', '6a5a7a8a'],
            '√Çge/Sexe': ['4H', '5M', '3F', '6H', '4M', '5H', '4F']
        })
    elif data_type == "attele":
        return pd.DataFrame({
            'Nom': ['Rapide √âclair', 'Foudre Noire', 'Vent du Nord', 'Temp√™te Rouge', 'Orage Bleu', 'Cyclone Vert'],
            'Num√©ro de corde': ['1', '2', '3', '4', '5', '6'],
            'Cote': ['4.2', '8.5', '15.0', '3.8', '6.8', '10.2'],
            'Poids': ['68.0', '68.0', '68.0', '68.0', '68.0', '68.0'],
            'Musique': ['2a1a4a1a', '4a3a2a5a', '6a5a8a7a', '1a2a1a3a', '3a4a5a2a', '5a6a4a8a'],
            '√Çge/Sexe': ['5H', '6M', '4F', '7H', '5M', '6H']
        })
    else:
        return pd.DataFrame({
            'Nom': ['Ace Impact', 'Torquator Tasso', 'Adayar', 'Tarnawa', 'Chrono Genesis', 'Mishriff', 'Love'],
            'Num√©ro de corde': ['1', '2', '3', '4', '5', '6', '7'],
            'Cote': ['3.2', '4.8', '7.5', '6.2', '9.1', '5.5', '11.0'],
            'Poids': ['59.5', '59.5', '59.5', '58.5', '58.5', '59.0', '58.0'],
            'Musique': ['1a1a2a1a', '1a3a1a2a', '2a1a4a1a', '1a2a1a3a', '3a1a2a1a', '1a1a1a2a', '2a3a1a4a'],
            '√Çge/Sexe': ['4H', '5H', '4H', '5F', '5F', '5H', '4F']
        })

def main():
    st.markdown("""
    <style>
    .main-header {
        font-size: 2.5rem;
        color: #1e3a8a;
        text-align: center;
        margin-bottom: 1rem;
        text-shadow: 2px 2px 4px rgba(0,0,0,0.1);
    }
    .url-input {
        background-color: #f8f9fa;
        padding: 20px;
        border-radius: 10px;
        border: 2px solid #e9ecef;
        margin-bottom: 20px;
    }
    </style>
    """, unsafe_allow_html=True)
    
    st.markdown('<h1 class="main-header">üèá Syst√®me Pr√©dictif Hippique Avanc√©</h1>', unsafe_allow_html=True)
    st.markdown("*Apprentissage automatique avec pond√©ration automatique des facteurs*")
    
    # Sidebar
    with st.sidebar:
        st.header("‚öôÔ∏è Configuration Avanc√©e")
        
        race_type = st.selectbox(
            "üèÅ Type de course",
            ["AUTO", "PLAT", "ATTELE_AUTOSTART", "ATTELE_VOLTE", "OBSTACLE"]
        )
        
        ml_method = st.selectbox(
            "üß† M√©thode ML",
            ["Ensemble Stacking", "XGBoost", "Random Forest", "R√©seau de Neurones"]
        )
        
        st.subheader("üìä Param√®tres d'Analyse")
        enable_correlation = st.checkbox("üìà Analyse de corr√©lation", value=True)
        enable_feature_importance = st.checkbox("üîç Importance des features", value=True)
        confidence_threshold = st.slider("üéØ Seuil de confiance", 0.5, 0.95, 0.7, 0.05)
    
    # SECTION URL
    st.markdown("---")
    st.header("üîç Analyse d'URL de Course")
    
    with st.container():
        st.markdown('<div class="url-input">', unsafe_allow_html=True)
        
        col1, col2 = st.columns([3, 1])
        with col1:
            url = st.text_input(
                "üåê **URL de la course:**",
                placeholder="https://example-racing-site.com/course/123",
                key="url_input"
            )
        with col2:
            st.markdown("<br>", unsafe_allow_html=True)
            analyze_button = st.button("üîç Analyser", type="primary", use_container_width=True)
        
        st.markdown('</div>', unsafe_allow_html=True)
    
    df_final = None
    
    # Traitement de l'URL
    if analyze_button and url:
        with st.spinner("üîÑ Extraction des donn√©es en cours..."):
            df, message = scrape_race_data(url)
            if df is not None:
                st.success(f"‚úÖ {len(df)} chevaux extraits avec succ√®s")
                st.dataframe(df.head(), use_container_width=True)
                df_final = df
            else:
                st.error(f"‚ùå {message}")
    
    # Autres onglets
    tab1, tab2 = st.tabs(["üìÅ Upload CSV", "üß™ Donn√©es de Test"])
    
    with tab1:
        st.subheader("üì§ Upload de fichier CSV")
        st.markdown("Format attendu: `Nom, Num√©ro de corde, Cote, Poids, Musique, √Çge/Sexe`")
        uploaded_file = st.file_uploader("Choisir un fichier CSV", type="csv", key="csv_uploader")
        if uploaded_file:
            try:
                df_final = pd.read_csv(uploaded_file)
                st.success(f"‚úÖ {len(df_final)} chevaux charg√©s")
                st.dataframe(df_final.head(), use_container_width=True)
            except Exception as e:
                st.error(f"‚ùå Erreur de lecture: {e}")
    
    with tab2:
        st.subheader("üß™ Donn√©es de Test")
        st.markdown("Tester l'analyseur avec des donn√©es pr√©-charg√©es")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if st.button("üèÉ Test Plat", use_container_width=True):
                df_final = generate_sample_data("plat")
                st.success("‚úÖ Donn√©es PLAT charg√©es (7 chevaux)")
        with col2:
            if st.button("üöó Test Attel√©", use_container_width=True):
                df_final = generate_sample_data("attele")
                st.success("‚úÖ Donn√©es ATTEL√â charg√©es (6 chevaux)")
        with col3:
            if st.button("‚≠ê Test Premium", use_container_width=True):
                df_final = generate_sample_data("premium")
                st.success("‚úÖ Donn√©es PREMIUM charg√©es (7 chevaux)")
        
        if df_final is not None:
            st.dataframe(df_final, use_container_width=True)
    
    # === ANALYSE PRINCIPALE ===
    if df_final is not None and len(df_final) > 0:
        st.markdown("---")
        st.header("üéØ Analyse et Pr√©dictions ML")
        
        df_prepared = prepare_enhanced_data(df_final)
        if len(df_prepared) == 0:
            st.error("‚ùå Aucune donn√©e valide apr√®s pr√©paration")
            return
        
        # Affichage info dataset
        st.info(f"üìä **Dataset**: {len(df_prepared)} chevaux | Cotes: {df_prepared['odds_numeric'].min():.1f}-{df_prepared['odds_numeric'].max():.1f}")
        
        # D√©tection du type de course
        if race_type == "AUTO":
            weight_std = df_prepared['weight_kg'].std()
            if weight_std > 2.5:
                detected_type = "PLAT"
                reason = "Grande variation de poids (handicap)"
            elif df_prepared['weight_kg'].mean() > 65 and weight_std < 1.5:
                detected_type = "ATTELE_AUTOSTART"
                reason = "Poids uniformes √©lev√©s (attel√©)"
            else:
                detected_type = "PLAT"
                reason = "Configuration par d√©faut"
            
            st.info(f"ü§ñ **Type d√©tect√©**: {detected_type} | **Raison**: {reason}")
        else:
            detected_type = race_type
            st.info(f"üìã **Type s√©lectionn√©**: {RACE_CONFIGS[detected_type]['description']}")
        
        # === MACHINE LEARNING AVANC√â ===
        predictor = AdvancedRacingPredictor()
        
        with st.spinner("ü§ñ Entra√Ænement des mod√®les ML adaptatifs..."):
            try:
                # Pr√©paration des features avanc√©es
                X_ml = predictor.prepare_advanced_features(df_prepared, detected_type)
                
                # Affichage info features
                st.info(f"üî¨ **{len(X_ml.columns)} features** cr√©√©es | **Samples**: {len(X_ml)}")
                
                # Pr√©diction avec confiance
                ml_predictions, ml_results, confidence_scores = predictor.predict_with_confidence(X_ml, detected_type)
                
                # Normalisation des pr√©dictions
                if len(ml_predictions) > 0 and ml_predictions.max() != ml_predictions.min():
                    ml_predictions = (ml_predictions - ml_predictions.min()) / (ml_predictions.max() - ml_predictions.min())
                
                df_prepared['ml_score'] = ml_predictions
                df_prepared['confidence'] = confidence_scores
                
                st.success("‚úÖ Mod√®les ML entra√Æn√©s avec succ√®s")
                
                # Affichage des m√©triques ML
                if ml_results:
                    cols = st.columns(min(4, len(ml_results)))
                    for idx, (name, scores) in enumerate(ml_results.items()):
                        with cols[idx % len(cols)]:
                            st.metric(f"üéØ R¬≤ {name}", f"{scores['mean_r2']:.3f}")
                    
                    # Confiance moyenne
                    avg_confidence = confidence_scores.mean()
                    st.metric("üìä Confiance Moyenne", f"{avg_confidence:.1%}")
                
            except Exception as e:
                st.error(f"‚ö†Ô∏è Erreur ML: {e}")
                # Fallback vers m√©thode simple
                traditional_score = 1 / (df_prepared['odds_numeric'] + 0.1)
                if traditional_score.max() != traditional_score.min():
                    traditional_score = (traditional_score - traditional_score.min()) / (traditional_score.max() - traditional_score.min())
                df_prepared['ml_score'] = traditional_score
                df_prepared['confidence'] = np.ones(len(df_prepared)) * 0.5
        
        # === SCORE FINAL ===
        ml_weight = 0.7
        df_prepared['score_final'] = (1 - ml_weight) * (1 / (df_prepared['odds_numeric'] + 0.1)) + ml_weight * df_prepared['ml_score']
        
        # === CLASSEMENT ===
        df_ranked = df_prepared.sort_values('score_final', ascending=False).reset_index(drop=True)
        df_ranked['rang'] = range(1, len(df_ranked) + 1)
        
        # === AFFICHAGE DES R√âSULTATS ===
        st.markdown("---")
        st.subheader("üèÜ Classement Final avec Confiance")
        
        # Pr√©paration affichage
        display_cols = ['rang', 'Nom', 'Cote', 'Num√©ro de corde', 'Poids', 'score_final', 'confidence']
        display_df = df_ranked[[col for col in display_cols if col in df_ranked.columns]].copy()
        
        # Formatage
        if 'score_final' in display_df.columns:
            display_df['Score'] = display_df['score_final'].apply(lambda x: f"{x:.3f}")
        if 'confidence' in display_df.columns:
            display_df['Confiance'] = display_df['confidence'].apply(lambda x: f"{x:.1%}")
        
        st.dataframe(display_df, use_container_width=True)

if __name__ == "__main__":
    main()
