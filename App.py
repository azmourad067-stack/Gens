import streamlit as st
import pandas as pd
import numpy as np
...
        üëÜ **Chargez des donn√©es pour voir le nouveau syst√®me en action !**
        """)

if __name__ == "__main__":
    main()
Tool Call
Function Name:
Write
Arguments:
file_path:
/home/user/pronostic_hippique_advanced_ml.py
content:
import streamlit as st
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup
import re
import json
from datetime import datetime, timedelta
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# ML avanc√© avec Deep Learning
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor
from sklearn.linear_model import Ridge, ElasticNet, BayesianRidge
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler
from sklearn.model_selection import cross_val_score, KFold, TimeSeriesSplit
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, accuracy_score
from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import scipy.stats as stats
from scipy.optimize import minimize
import warnings
warnings.filterwarnings('ignore')

# Configuration avanc√©e
tf.random.set_seed(42)
np.random.seed(42)

st.set_page_config(
    page_title="üèá Analyseur Hippique IA Deep Learning Pro",
    page_icon="üèá",
    layout="wide"
)

st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        color: #1e3a8a;
        text-align: center;
        margin-bottom: 2rem;
        text-shadow: 2px 2px 4px rgba(0,0,0,0.1);
    }
    .metric-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1.2rem;
        border-radius: 12px;
        color: white;
        text-align: center;
        margin: 0.5rem 0;
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
    }
    .prediction-box {
        border-left: 5px solid #f59e0b;
        padding: 1rem 1rem 1rem 1.5rem;
        background: linear-gradient(90deg, #fffbeb 0%, #ffffff 100%);
        margin: 1rem 0;
        border-radius: 8px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.05);
    }
    .confidence-high { color: #10b981; font-weight: bold; }
    .confidence-medium { color: #f59e0b; font-weight: bold; }
    .confidence-low { color: #ef4444; font-weight: bold; }
    .neural-network {
        background: linear-gradient(45deg, #ff6b6b, #4ecdc4);
        color: white;
        padding: 1rem;
        border-radius: 10px;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

# Configurations avanc√©es pour diff√©rents types de courses
RACE_CONFIGS = {
    "PLAT": {
        "description": "üèÉ Course de galop - Handicap poids + avantage corde int√©rieure",
        "optimal_draws": [1, 2, 3, 4, 5],
        "weight_importance": 0.35,
        "distance_factor": 1.2,
        "age_optimal": [3, 4, 5, 6],
        "track_bias": "inner"
    },
    "ATTELE_AUTOSTART": {
        "description": "üöó Trot attel√© autostart - Num√©ros 4-6 optimaux", 
        "optimal_draws": [4, 5, 6, 7],
        "weight_importance": 0.05,
        "distance_factor": 1.0,
        "age_optimal": [4, 5, 6, 7, 8],
        "track_bias": "middle"
    },
    "ATTELE_VOLTE": {
        "description": "üîÑ Trot attel√© volt√© - Num√©ro sans importance",
        "optimal_draws": [],
        "weight_importance": 0.05,
        "distance_factor": 1.1,
        "age_optimal": [4, 5, 6, 7, 8, 9],
        "track_bias": "neutral"
    },
    "OBSTACLE": {
        "description": "üöß Course d'obstacles - Exp√©rience cruciale",
        "optimal_draws": [2, 3, 4, 5],
        "weight_importance": 0.25,
        "distance_factor": 1.5,
        "age_optimal": [6, 7, 8, 9, 10],
        "track_bias": "experience"
    }
}

class AdvancedNeuralHorseRacingML:
    def __init__(self):
        # Mod√®les de base
        self.base_models = {}
        self.neural_model = None
        self.ensemble_model = None
        
        # Scalers et preprocessors
        self.feature_scaler = RobustScaler()
        self.target_scaler = MinMaxScaler()
        self.feature_selector = None
        self.pca = PCA(n_components=0.95)  # Garder 95% de la variance
        
        # M√©triques et √©valuations
        self.feature_importance = {}
        self.cv_scores = {}
        self.correlation_matrix = None
        self.feature_correlations = {}
        self.model_weights = {}
        
        # Param√®tres d'apprentissage
        self.is_trained = False
        self.training_history = {}
        self.feature_names = []
        
        # Configuration probabiliste
        self.bayesian_model = None
        self.uncertainty_estimates = {}
        
    def create_advanced_neural_model(self, input_dim, race_type="PLAT"):
        """Cr√©ation d'un r√©seau de neurones avanc√© adaptatif"""
        
        # Architecture adaptative selon le type de course
        if race_type == "PLAT":
            hidden_layers = [128, 64, 32, 16]
            dropout_rate = 0.3
            l2_reg = 0.001
        elif "ATTELE" in race_type:
            hidden_layers = [96, 48, 24, 12]
            dropout_rate = 0.2
            l2_reg = 0.0005
        else:
            hidden_layers = [112, 56, 28, 14]
            dropout_rate = 0.25
            l2_reg = 0.0008
        
        model = keras.Sequential([
            # Couche d'entr√©e avec normalisation
            layers.Dense(hidden_layers[0], input_dim=input_dim, 
                        activation='relu',
                        kernel_regularizer=keras.regularizers.l2(l2_reg),
                        kernel_initializer='he_normal'),
            layers.BatchNormalization(),
            layers.Dropout(dropout_rate),
            
            # Couches cach√©es avec skip connections simul√©es
            layers.Dense(hidden_layers[1], activation='relu',
                        kernel_regularizer=keras.regularizers.l2(l2_reg)),
            layers.BatchNormalization(),
            layers.Dropout(dropout_rate * 0.8),
            
            layers.Dense(hidden_layers[2], activation='relu',
                        kernel_regularizer=keras.regularizers.l2(l2_reg)),
            layers.BatchNormalization(),
            layers.Dropout(dropout_rate * 0.6),
            
            layers.Dense(hidden_layers[3], activation='relu',
                        kernel_regularizer=keras.regularizers.l2(l2_reg)),
            layers.BatchNormalization(),
            layers.Dropout(dropout_rate * 0.4),
            
            # Couches de sortie avec incertitude
            layers.Dense(8, activation='relu'),
            layers.Dense(1, activation='sigmoid')  # Probabilit√© de victoire
        ])
        
        # Optimizer adaptatif
        optimizer = keras.optimizers.Adam(
            learning_rate=0.001,
            beta_1=0.9,
            beta_2=0.999,
            epsilon=1e-7
        )
        
        model.compile(
            optimizer=optimizer,
            loss='mse',
            metrics=['mae', 'mse']
        )
        
        return model
    
    def create_bayesian_model(self):
        """Mod√®le bay√©sien pour l'estimation d'incertitude"""
        self.bayesian_model = BayesianRidge(
            n_iter=500,
            tol=1e-3,
            alpha_1=1e-6,
            alpha_2=1e-6,
            lambda_1=1e-6,
            lambda_2=1e-6,
            compute_score=True,
            fit_intercept=True
        )
    
    def extract_advanced_music_features(self, music_str, race_type="PLAT"):
        """Extraction ultra-avanc√©e des performances avec pond√©ration contextuelle"""
        if pd.isna(music_str) or music_str == '':
            return self._default_music_features()
        
        music = str(music_str).upper()
        
        # Extraction des positions num√©riques et lettres
        positions = []
        conditions = []
        
        i = 0
        while i < len(music):
            if music[i].isdigit():
                # Position num√©rique
                num = int(music[i])
                if num > 0:
                    positions.append(num)
                    # Chercher les conditions apr√®s le chiffre
                    j = i + 1
                    cond = ""
                    while j < len(music) and music[j].isalpha():
                        cond += music[j]
                        j += 1
                    conditions.append(cond)
                    i = j
            else:
                i += 1
        
        if not positions:
            return self._default_music_features()
        
        # Calculs de base
        total = len(positions)
        wins = positions.count(1)
        places = sum(1 for p in positions if p <= 3)
        
        # Analyse de la forme r√©cente (pond√©ration d√©croissante)
        recent_positions = positions[:min(5, len(positions))]
        form_weights = [0.4, 0.25, 0.2, 0.1, 0.05][:len(recent_positions)]
        recent_form = sum(w * (1/p if p <= 10 else 0) for w, p in zip(form_weights, recent_positions))
        
        # R√©gularit√© et variabilit√©
        position_variance = np.var(positions) if len(positions) > 1 else 0
        consistency_score = 1 / (1 + position_variance)
        
        # Analyse des conditions de course
        condition_bonus = 0
        for cond in conditions[:3]:  # 3 derni√®res courses
            if 'A' in cond:  # Conditions automatique/favorable
                condition_bonus += 0.1
            elif 'P' in cond:  # P√©nalit√©
                condition_bonus -= 0.05
            elif 'D' in cond:  # Disqualifi√©
                condition_bonus -= 0.1
        
        # M√©triques de progression
        if len(positions) >= 3:
            early_avg = np.mean(positions[-3:])  # 3 derni√®res
            late_avg = np.mean(positions[:3])    # 3 premi√®res
            progression = (early_avg - late_avg) / early_avg if early_avg > 0 else 0
        else:
            progression = 0
        
        # Adaptation selon le type de course
        race_factor = RACE_CONFIGS.get(race_type, RACE_CONFIGS["PLAT"])
        
        # Bonus pour l'exp√©rience dans les obstacles
        experience_bonus = 0
        if race_type == "OBSTACLE" and total >= 5:
            experience_bonus = min(0.2, total * 0.02)
        
        return {
            'wins': wins,
            'places': places,
            'total_races': total,
            'win_rate': wins / total if total > 0 else 0,
            'place_rate': places / total if total > 0 else 0,
            'consistency': consistency_score,
            'recent_form': recent_form,
            'best_position': min(positions),
            'avg_position': np.mean(positions),
            'position_variance': position_variance,
            'progression_trend': progression,
            'condition_adjustment': condition_bonus,
            'experience_factor': experience_bonus,
            'strike_rate': wins / max(1, total),
            'top3_consistency': places / max(1, total),
            'recent_win_rate': sum(1 for p in recent_positions if p == 1) / len(recent_positions) if recent_positions else 0,
            'distance_adaptability': 1 - (position_variance / max(1, np.mean(positions))),
        }
    
    def _default_music_features(self):
        """Features par d√©faut pour chevaux sans historique"""
        return {
            'wins': 0, 'places': 0, 'total_races': 0,
            'win_rate': 0, 'place_rate': 0, 'consistency': 0,
            'recent_form': 0, 'best_position': 10,
            'avg_position': 8, 'position_variance': 5,
            'progression_trend': 0, 'condition_adjustment': 0,
            'experience_factor': 0, 'strike_rate': 0,
            'top3_consistency': 0, 'recent_win_rate': 0,
            'distance_adaptability': 0
        }
    
    def create_ultra_advanced_features(self, df, race_type="PLAT"):
        """Cr√©ation de features ultra-avanc√©es avec ing√©nierie pouss√©e"""
        features = pd.DataFrame()
        config = RACE_CONFIGS.get(race_type, RACE_CONFIGS["PLAT"])
        
        # === FEATURES DE COTES SOPHISTIQU√âES ===
        odds = df['odds_numeric']
        
        # Transformations math√©matiques des cotes
        features['odds_log'] = np.log1p(odds)
        features['odds_inv'] = 1 / (odds + 0.01)
        features['odds_sqrt'] = np.sqrt(odds)
        features['odds_power'] = np.power(odds, 0.3)  # Transformation Box-Cox approxim√©e
        
        # M√©triques relatives de cotes
        features['odds_z_score'] = (odds - odds.mean()) / (odds.std() + 1e-6)
        features['odds_rank'] = odds.rank(method='min')
        features['odds_percentile'] = odds.rank(pct=True)
        features['odds_relative_to_fav'] = odds / odds.min()
        
        # Classification des chevaux
        features['is_favorite'] = (odds == odds.min()).astype(int)
        features['is_second_favorite'] = (odds.rank() == 2).astype(int)
        features['is_outsider'] = (odds > odds.quantile(0.8)).astype(int)
        features['is_longshot'] = (odds > 20).astype(int)
        
        # Mesures de comp√©titivit√©
        features['competitive_index'] = odds.std() / (odds.mean() + 1e-6)
        features['field_strength'] = 1 / odds.mean()
        features['market_confidence'] = 1 - (odds.std() / odds.mean())
        
        # === FEATURES DE POSITION AVANC√âES ===
        draws = df['draw_numeric']
        
        # Position brute et transformations
        features['draw'] = draws
        features['draw_normalized'] = draws / draws.max()
        features['draw_centered'] = draws - draws.mean()
        features['draw_from_rail'] = draws - 1  # Distance de la corde
        
        # Avantages positionnels selon le type de course
        optimal_draws = config['optimal_draws']
        if optimal_draws:
            features['optimal_position'] = draws.apply(lambda x: 1 if x in optimal_draws else 0)
            features['distance_to_optimal'] = draws.apply(
                lambda x: min([abs(x - opt) for opt in optimal_draws])
            )
            features['position_advantage'] = draws.apply(
                lambda x: max(0, 1 - min([abs(x - opt) for opt in optimal_draws]) / 5)
            )
        else:
            features['optimal_position'] = 0.5  # Neutre
            features['distance_to_optimal'] = 0
            features['position_advantage'] = 0.5
        
        # Biais de piste selon le type
        if config['track_bias'] == 'inner':
            features['track_bias_advantage'] = 1 / draws  # Plus proche = mieux
        elif config['track_bias'] == 'middle':
            features['track_bias_advantage'] = 1 / (abs(draws - draws.mean()) + 1)
        else:
            features['track_bias_advantage'] = 0.5  # Neutre
        
        # === FEATURES DE POIDS SOPHISTIQU√âES ===
        weights = df['weight_kg']
        weight_importance = config['weight_importance']
        
        # M√©triques de poids
        features['weight'] = weights
        features['weight_z_score'] = (weights - weights.mean()) / (weights.std() + 1e-6)
        features['weight_rank'] = weights.rank(method='min')
        features['weight_advantage'] = (weights.max() - weights) * weight_importance
        features['weight_penalty'] = (weights - weights.min()) * weight_importance
        
        # Optimisation du poids
        features['weight_optimal'] = weights.apply(
            lambda w: 1 - abs(w - weights.median()) / (weights.std() + 1)
        )
        
        # === FEATURES D'√ÇGE ET SEXE AVANC√âES ===
        if '√Çge/Sexe' in df.columns:
            age_sex = df['√Çge/Sexe'].fillna('5H')
            ages = age_sex.str.extract('(\d+)').astype(float).fillna(5)[0]
            
            features['age'] = ages
            features['age_squared'] = ages ** 2
            features['age_cubed'] = ages ** 3
            
            # Courbe d'√¢ge optimal selon le type de course
            optimal_ages = config['age_optimal']
            features['age_optimal'] = ages.apply(lambda a: 1 if a in optimal_ages else 0)
            features['age_factor'] = ages.apply(
                lambda a: max(0, 1 - min([abs(a - opt) for opt in optimal_ages]) / 3)
            )
            
            # Sexe
            features['is_mare'] = age_sex.str.contains('F', na=False).astype(int)
            features['is_stallion'] = age_sex.str.contains('H', na=False).astype(int)
            features['is_gelding'] = age_sex.str.contains('M', na=False).astype(int)
            
            # Interactions √¢ge-sexe
            features['young_mare'] = ((ages <= 4) & age_sex.str.contains('F', na=False)).astype(int)
            features['mature_stallion'] = ((ages >= 5) & age_sex.str.contains('H', na=False)).astype(int)
            
        else:
            # Valeurs par d√©faut
            features['age'] = 5
            features['age_squared'] = 25
            features['age_cubed'] = 125
            features['age_optimal'] = 1
            features['age_factor'] = 1
            features['is_mare'] = 0
            features['is_stallion'] = 1
            features['is_gelding'] = 0
            features['young_mare'] = 0
            features['mature_stallion'] = 1
        
        # === FEATURES MUSICALES ULTRA-AVANC√âES ===
        if 'Musique' in df.columns:
            music_features = df['Musique'].apply(
                lambda x: self.extract_advanced_music_features(x, race_type)
            )
            for key in music_features.iloc[0].keys():
                features[f'music_{key}'] = [m[key] for m in music_features]
        else:
            default_music = self._default_music_features()
            for key, value in default_music.items():
                features[f'music_{key}'] = value
        
        # M√©triques d√©riv√©es de la musique
        features['music_class_rating'] = (
            features['music_win_rate'] * 0.4 +
            features['music_place_rate'] * 0.3 +
            features['music_recent_form'] * 0.3
        )
        
        # === FEATURES D'INTERACTION COMPLEXES ===
        
        # Interactions cotes-performance
        features['odds_music_quality'] = features['odds_inv'] * features['music_class_rating']
        features['favorite_with_form'] = features['is_favorite'] * features['music_recent_form']
        features['outsider_with_class'] = features['is_outsider'] * features['music_win_rate']
        
        # Interactions position-caract√©ristiques
        features['position_weight_penalty'] = features['draw'] * features['weight_penalty']
        features['optimal_draw_with_form'] = features['optimal_position'] * features['music_recent_form']
        
        # Interactions √¢ge-performance
        features['age_experience'] = features['age'] * features['music_total_races']
        features['young_talent'] = (features['age'] <= 4) * features['music_win_rate']
        features['veteran_consistency'] = (features['age'] >= 7) * features['music_consistency']
        
        # === FEATURES DE CLUSTERING ET PATTERNS ===
        
        # Clustering bas√© sur les performances
        cluster_features = ['odds_inv', 'music_win_rate', 'music_consistency', 'age_factor']
        if len(df) >= 3:
            try:
                kmeans = KMeans(n_clusters=min(3, len(df)), random_state=42)
                cluster_data = features[cluster_features].fillna(0)
                features['performance_cluster'] = kmeans.fit_predict(cluster_data)
                features['cluster_center_distance'] = np.min(
                    kmeans.transform(cluster_data), axis=1
                )
            except:
                features['performance_cluster'] = 0
                features['cluster_center_distance'] = 0
        else:
            features['performance_cluster'] = 0
            features['cluster_center_distance'] = 0
        
        # === FEATURES STATISTIQUES AVANC√âES ===
        
        # Entropie et diversit√©
        if len(df) > 1:
            prob_dist = features['odds_inv'] / features['odds_inv'].sum()
            features['market_entropy'] = -np.sum(prob_dist * np.log2(prob_dist + 1e-10))
        else:
            features['market_entropy'] = 0
        
        # M√©triques de contexte de course
        features['field_size'] = len(df)
        features['field_size_normalized'] = min(1, len(df) / 20)  # Normalisation sur 20 chevaux max
        
        # Score composite final
        features['composite_score'] = (
            features['odds_inv'] * 0.25 +
            features['music_class_rating'] * 0.25 +
            features['position_advantage'] * 0.15 +
            features['age_factor'] * 0.10 +
            (1 - features['weight_penalty']) * 0.15 +
            features['music_progression_trend'] * 0.10
        )
        
        # Nettoyage et normalisation finale
        features = features.fillna(features.median()).replace([np.inf, -np.inf], 0)
        
        return features
    
    def calculate_feature_correlations(self, X, y):
        """Calcul des corr√©lations entre features et target"""
        correlations = {}
        
        for col in X.columns:
            try:
                # Corr√©lation de Pearson
                pearson_corr, p_val = stats.pearsonr(X[col], y)
                
                # Corr√©lation de Spearman (non-param√©trique)
                spearman_corr, _ = stats.spearmanr(X[col], y)
                
                # Information mutuelle
                mi_score = mutual_info_regression(X[[col]], y)[0]
                
                correlations[col] = {
                    'pearson': abs(pearson_corr),
                    'spearman': abs(spearman_corr),
                    'mutual_info': mi_score,
                    'p_value': p_val,
                    'composite_score': (abs(pearson_corr) + abs(spearman_corr) + mi_score) / 3
                }
            except:
                correlations[col] = {
                    'pearson': 0, 'spearman': 0, 'mutual_info': 0,
                    'p_value': 1, 'composite_score': 0
                }
        
        return correlations
    
    def optimize_model_weights(self, predictions_dict, y_true):
        """Optimisation automatique des poids des mod√®les"""
        
        def objective(weights):
            weights = weights / np.sum(weights)  # Normalisation
            ensemble_pred = sum(
                w * predictions_dict[model] 
                for w, model in zip(weights, predictions_dict.keys())
            )
            return mean_squared_error(y_true, ensemble_pred)
        
        # Contraintes: tous les poids positifs et somme = 1
        n_models = len(predictions_dict)
        constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}
        bounds = [(0, 1) for _ in range(n_models)]
        
        # Poids initiaux uniformes
        initial_weights = np.ones(n_models) / n_models
        
        try:
            result = minimize(
                objective, 
                initial_weights, 
                method='SLSQP',
                bounds=bounds,
                constraints=constraints
            )
            
            if result.success:
                optimal_weights = result.x / np.sum(result.x)
                return dict(zip(predictions_dict.keys(), optimal_weights))
            else:
                return dict(zip(predictions_dict.keys(), initial_weights))
                
        except:
            return dict(zip(predictions_dict.keys(), initial_weights))
    
    def train_advanced_ensemble(self, X, y, race_type="PLAT"):
        """Entra√Ænement d'ensemble ultra-avanc√© avec optimisation automatique"""
        
        if len(X) < 5:
            st.warning("‚ö†Ô∏è Donn√©es insuffisantes pour un entra√Ænement robuste")
            return np.random.random(len(X)), {}, np.ones(len(X)) * 0.3
        
        self.feature_names = X.columns.tolist()
        
        # === PREPROCESSING AVANC√â ===
        
        # 1. S√©lection de features par corr√©lation
        correlations = self.calculate_feature_correlations(X, y)
        self.feature_correlations = correlations
        
        # S√©lection des meilleures features
        feature_scores = [(col, corr['composite_score']) for col, corr in correlations.items()]
        feature_scores.sort(key=lambda x: x[1], reverse=True)
        
        n_features_to_keep = min(30, len(X.columns))  # Maximum 30 features
        best_features = [f[0] for f in feature_scores[:n_features_to_keep]]
        X_selected = X[best_features].copy()
        
        # 2. Normalisation robuste
        X_scaled = self.feature_scaler.fit_transform(X_selected)
        X_scaled = pd.DataFrame(X_scaled, columns=best_features)
        
        # 3. R√©duction de dimensionnalit√© si n√©cessaire
        if len(best_features) > 20:
            try:
                X_pca = self.pca.fit_transform(X_scaled)
                pca_features = [f'PCA_{i}' for i in range(X_pca.shape[1])]
                X_final = pd.DataFrame(X_pca, columns=pca_features)
            except:
                X_final = X_scaled
        else:
            X_final = X_scaled
        
        # === CR√âATION DES MOD√àLES DE BASE ===
        
        self.base_models = {
            'random_forest': RandomForestRegressor(
                n_estimators=300,
                max_depth=10,
                min_samples_split=5,
                min_samples_leaf=2,
                max_features='sqrt',
                random_state=42,
                n_jobs=-1
            ),
            'gradient_boosting': GradientBoostingRegressor(
                n_estimators=200,
                learning_rate=0.05,
                max_depth=6,
                min_samples_split=8,
                subsample=0.8,
                random_state=42
            ),
            'ridge': Ridge(alpha=0.5, random_state=42),
            'elastic': ElasticNet(alpha=0.3, l1_ratio=0.7, random_state=42)
        }
        
        # Mod√®le bay√©sien pour l'incertitude
        self.create_bayesian_model()
        
        # === CR√âATION DU R√âSEAU DE NEURONES ===
        
        try:
            self.neural_model = self.create_advanced_neural_model(X_final.shape[1], race_type)
            
            # Callbacks pour l'entra√Ænement
            early_stopping = keras.callbacks.EarlyStopping(
                monitor='loss',
                patience=20,
                restore_best_weights=True
            )
            
            reduce_lr = keras.callbacks.ReduceLROnPlateau(
                monitor='loss',
                factor=0.5,
                patience=10,
                min_lr=1e-6
            )
            
            # Entra√Ænement du r√©seau de neurones
            history = self.neural_model.fit(
                X_final.values, y,
                epochs=100,
                batch_size=max(2, len(X_final) // 4),
                verbose=0,
                callbacks=[early_stopping, reduce_lr],
                validation_split=0.2 if len(X_final) > 10 else 0
            )
            
            self.training_history = history.history
            neural_predictions = self.neural_model.predict(X_final.values, verbose=0).flatten()
            
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Erreur r√©seau de neurones: {e}")
            neural_predictions = np.random.random(len(X_final))
        
        # === ENTRA√éNEMENT DES MOD√àLES CLASSIQUES ===
        
        predictions_dict = {}
        
        # Validation crois√©e temporelle si possible
        if len(X_final) >= 10:
            cv = TimeSeriesSplit(n_splits=min(5, len(X_final) // 2))
        else:
            cv = KFold(n_splits=min(3, len(X_final)), shuffle=True, random_state=42)
        
        for name, model in self.base_models.items():
            try:
                # Validation crois√©e
                scores = cross_val_score(model, X_final, y, cv=cv, scoring='neg_mean_squared_error')
                self.cv_scores[name] = {
                    'mean': -scores.mean(),
                    'std': scores.std(),
                    'scores': -scores
                }
                
                # Entra√Ænement complet
                model.fit(X_final, y)
                pred = model.predict(X_final)
                predictions_dict[name] = pred
                
                # Importance des features
                if hasattr(model, 'feature_importances_'):
                    importance = dict(zip(X_final.columns, model.feature_importances_))
                    self.feature_importance[name] = dict(
                        sorted(importance.items(), key=lambda x: x[1], reverse=True)[:15]
                    )
                elif hasattr(model, 'coef_'):
                    importance = dict(zip(X_final.columns, abs(model.coef_)))
                    self.feature_importance[name] = dict(
                        sorted(importance.items(), key=lambda x: x[1], reverse=True)[:15]
                    )
                    
            except Exception as e:
                st.warning(f"‚ö†Ô∏è Erreur mod√®le {name}: {e}")
                predictions_dict[name] = np.random.random(len(X_final))
                self.cv_scores[name] = {'mean': 1.0, 'std': 0.5, 'scores': [1.0]}
        
        # Entra√Ænement du mod√®le bay√©sien
        try:
            self.bayesian_model.fit(X_final, y)
            bayesian_pred, bayesian_std = self.bayesian_model.predict(X_final, return_std=True)
            predictions_dict['bayesian'] = bayesian_pred
            self.uncertainty_estimates = {
                'bayesian_std': bayesian_std,
                'mean_uncertainty': np.mean(bayesian_std)
            }
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Erreur mod√®le bay√©sien: {e}")
            predictions_dict['bayesian'] = np.random.random(len(X_final))
            self.uncertainty_estimates = {'bayesian_std': np.ones(len(X_final)) * 0.5}
        
        # Ajout des pr√©dictions neuronales
        predictions_dict['neural_network'] = neural_predictions
        
        # === OPTIMISATION AUTOMATIQUE DES POIDS ===
        
        optimal_weights = self.optimize_model_weights(predictions_dict, y)
        self.model_weights = optimal_weights
        
        # === PR√âDICTION FINALE OPTIMIS√âE ===
        
        final_predictions = sum(
            predictions_dict[model] * weight
            for model, weight in optimal_weights.items()
        )
        
        # Normalisation finale
        if final_predictions.max() != final_predictions.min():
            final_predictions = (final_predictions - final_predictions.min()) / \
                              (final_predictions.max() - final_predictions.min())
        
        # === CALCUL DE LA CONFIANCE AVANC√âE ===
        
        # Confiance bas√©e sur:
        # 1. Variance des pr√©dictions entre mod√®les
        pred_matrix = np.column_stack(list(predictions_dict.values()))
        prediction_variance = np.var(pred_matrix, axis=1)
        
        # 2. Incertitude bay√©sienne
        bayesian_uncertainty = self.uncertainty_estimates.get('bayesian_std', np.ones(len(X_final)) * 0.5)
        
        # 3. Qualit√© des features
        feature_quality = 1 - (X_final.isna().sum(axis=1) / len(X_final.columns))
        
        # 4. Performance de validation crois√©e
        avg_cv_score = np.mean([scores['mean'] for scores in self.cv_scores.values()])
        cv_confidence = max(0, 1 - avg_cv_score)
        
        # Confiance composite
        confidence = (
            (1 / (1 + prediction_variance)) * 0.3 +
            (1 / (1 + bayesian_uncertainty)) * 0.3 +
            feature_quality * 0.2 +
            cv_confidence * 0.2
        )
        
        confidence = np.clip(confidence, 0.1, 0.95)
        
        self.is_trained = True
        
        return final_predictions, self.cv_scores, confidence

# Conservation des fonctions de scraping existantes (inchang√©es)
@st.cache_data(ttl=300)
def scrape_race_data(url):
    """Fonction de scraping inchang√©e - fonctionne parfaitement"""
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        response = requests.get(url, headers=headers, timeout=10)
        
        if response.status_code != 200:
            return None, f"Erreur HTTP {response.status_code}"

        soup = BeautifulSoup(response.content, 'html.parser')
        horses_data = []
        
        table = soup.find('table')
        if not table:
            return None, "Aucun tableau trouv√©"
            
        rows = table.find_all('tr')[1:]
        
        for row in rows:
            cols = row.find_all(['td', 'th'])
            if len(cols) >= 4:
                horses_data.append({
                    "Num√©ro de corde": cols[0].get_text(strip=True),
                    "Nom": cols[1].get_text(strip=True),
                    "Cote": cols[-1].get_text(strip=True),
                    "Poids": cols[-2].get_text(strip=True) if len(cols) > 4 else "60.0",
                    "Musique": cols[2].get_text(strip=True) if len(cols) > 5 else "",
                    "√Çge/Sexe": cols[3].get_text(strip=True) if len(cols) > 6 else "",
                })

        if not horses_data:
            return None, "Aucune donn√©e extraite"
            
        return pd.DataFrame(horses_data), "Succ√®s"
        
    except Exception as e:
        return None, f"Erreur: {str(e)}"

def safe_convert(value, convert_func, default=0):
    """Conversion s√©curis√©e inchang√©e"""
    try:
        if pd.isna(value):
            return default
        cleaned = str(value).replace(',', '.').strip()
        return convert_func(cleaned)
    except:
        return default

def prepare_data(df):
    """Pr√©paration de donn√©es inchang√©e"""
    df = df.copy()
    df['odds_numeric'] = df['Cote'].apply(lambda x: safe_convert(x, float, 999))
    df['draw_numeric'] = df['Num√©ro de corde'].apply(lambda x: safe_convert(x, int, 1))
    
    def extract_weight(poids_str):
        if pd.isna(poids_str):
            return 60.0
        match = re.search(r'(\d+(?:[.,]\d+)?)', str(poids_str))
        return float(match.group(1).replace(',', '.')) if match else 60.0
    
    df['weight_kg'] = df['Poids'].apply(extract_weight)
    df = df[df['odds_numeric'] > 0]
    df = df.reset_index(drop=True)
    return df

def auto_detect_race_type(df):
    """D√©tection automatique du type de course am√©lior√©e"""
    weight_std = df['weight_kg'].std()
    weight_mean = df['weight_kg'].mean()
    odds_range = df['odds_numeric'].max() - df['odds_numeric'].min()
    
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("üí™ √âcart-type poids", f"{weight_std:.1f} kg")
    with col2:
        st.metric("‚öñÔ∏è Poids moyen", f"{weight_mean:.1f} kg")
    with col3:
        st.metric("üèá Nb chevaux", len(df))
    with col4:
        st.metric("üìä √âcart cotes", f"{odds_range:.1f}")
    
    # Logique de d√©tection am√©lior√©e
    if weight_std > 3.0 and weight_mean < 65:
        detected = "PLAT"
        reason = "Grande variation de poids (handicap plat)"
    elif weight_std > 2.0 and weight_mean > 65:
        detected = "OBSTACLE"
        reason = "Poids √©lev√©s avec variation (obstacle)"
    elif weight_mean > 67 and weight_std < 1.0:
        detected = "ATTELE_AUTOSTART"
        reason = "Poids uniformes √©lev√©s (attel√© autostart)"
    elif weight_std < 1.5 and weight_mean > 65:
        detected = "ATTELE_VOLTE"
        reason = "Poids uniformes (attel√© volt√©)"
    else:
        detected = "PLAT"
        reason = "Configuration par d√©faut"
    
    st.info(f"ü§ñ **Type d√©tect√©**: {detected} | **Raison**: {reason}")
    return detected

def create_advanced_neural_visualization(df_ranked, ml_model=None):
    """Visualisations ultra-avanc√©es avec m√©triques de deep learning"""
    
    fig = make_subplots(
        rows=3, cols=3,
        subplot_titles=(
            'üß† Pr√©dictions Neural Network', 
            'üìä Distribution Confiance', 
            'üîó Corr√©lations Features',
            '‚öñÔ∏è Poids Mod√®les Optimis√©s', 
            'üìà Historique Entra√Ænement NN',
            'üéØ Incertitude Bay√©sienne',
            'üåê Clustering Performances',
            'üìâ Validation Crois√©e D√©taill√©e',
            'üî• Heatmap Importance'
        ),
        specs=[
            [{"secondary_y": False}, {"type": "histogram"}, {"type": "heatmap"}],
            [{"type": "bar"}, {"type": "scatter"}, {"type": "scatter"}],
            [{"type": "scatter"}, {"type": "bar"}, {"type": "heatmap"}]
        ]
    )
    
    colors = px.colors.qualitative.Set3
    
    # 1. Pr√©dictions du r√©seau de neurones avec confiance
    if 'score_final' in df_ranked.columns and 'confidence' in df_ranked.columns:
        fig.add_trace(
            go.Scatter(
                x=df_ranked.index,
                y=df_ranked['score_final'],
                mode='markers+lines',
                marker=dict(
                    size=df_ranked['confidence'] * 25,
                    color=df_ranked['confidence'],
                    colorscale='Viridis',
                    showscale=True,
                    colorbar=dict(title="Confiance", x=0.35)
                ),
                text=df_ranked['Nom'],
                name='Score Neural',
                line=dict(width=3)
            ), row=1, col=1
        )
    
    # 2. Distribution de la confiance
    if 'confidence' in df_ranked.columns:
        fig.add_trace(
            go.Histogram(
                x=df_ranked['confidence'],
                nbinsx=8,
                marker_color=colors[1],
                name='Confiance',
                opacity=0.7
            ), row=1, col=2
        )
    
    # 3. Heatmap des corr√©lations de features (si disponible)
    if ml_model and ml_model.feature_correlations:
        # Top 10 corr√©lations
        top_corr = sorted(
            ml_model.feature_correlations.items(),
            key=lambda x: x[1]['composite_score'],
            reverse=True
        )[:10]
        
        corr_names = [item[0][:15] for item in top_corr]  # Noms tronqu√©s
        corr_values = [[item[1]['composite_score']] for item in top_corr]
        
        fig.add_trace(
            go.Heatmap(
                z=corr_values,
                y=corr_names,
                x=['Corr√©lation'],
                colorscale='RdYlGn',
                showscale=True,
                colorbar=dict(title="Score", x=1.0)
            ), row=1, col=3
        )
    
    # 4. Poids des mod√®les optimis√©s
    if ml_model and ml_model.model_weights:
        models = list(ml_model.model_weights.keys())
        weights = list(ml_model.model_weights.values())
        
        fig.add_trace(
            go.Bar(
                x=models,
                y=weights,
                marker_color=colors[3],
                name='Poids Optimaux',
                text=[f'{w:.3f}' for w in weights],
                textposition='auto'
            ), row=2, col=1
        )
    
    # 5. Historique d'entra√Ænement du NN
    if ml_model and ml_model.training_history and 'loss' in ml_model.training_history:
        epochs = range(1, len(ml_model.training_history['loss']) + 1)
        fig.add_trace(
            go.Scatter(
                x=list(epochs),
                y=ml_model.training_history['loss'],
                mode='lines',
                name='Loss',
                line=dict(color=colors[4], width=2)
            ), row=2, col=2
        )
    
    # 6. Incertitude bay√©sienne
    if ml_model and ml_model.uncertainty_estimates:
        if 'bayesian_std' in ml_model.uncertainty_estimates:
            uncertainty = ml_model.uncertainty_estimates['bayesian_std']
            fig.add_trace(
                go.Scatter(
                    x=df_ranked.index,
                    y=uncertainty,
                    mode='markers',
                    marker=dict(
                        size=8,
                        color=uncertainty,
                        colorscale='Reds',
                        showscale=False
                    ),
                    text=df_ranked['Nom'],
                    name='Incertitude'
                ), row=2, col=3
            )
    
    # 7. Clustering des performances
    if 'score_final' in df_ranked.columns and 'odds_numeric' in df_ranked.columns:
        fig.add_trace(
            go.Scatter(
                x=df_ranked['odds_numeric'],
                y=df_ranked['score_final'],
                mode='markers',
                marker=dict(
                    size=12,
                    color=df_ranked.index,
                    colorscale='Viridis',
                    showscale=False
                ),
                text=df_ranked['Nom'],
                name='Clusters'
            ), row=3, col=1
        )
    
    # 8. D√©tails de la validation crois√©e
    if ml_model and ml_model.cv_scores:
        models = list(ml_model.cv_scores.keys())
        means = [ml_model.cv_scores[m]['mean'] for m in models]
        stds = [ml_model.cv_scores[m]['std'] for m in models]
        
        fig.add_trace(
            go.Bar(
                x=models,
                y=means,
                error_y=dict(type='data', array=stds, color='red'),
                marker_color=colors[7],
                name='MSE CV',
                text=[f'{m:.3f}¬±{s:.3f}' for m, s in zip(means, stds)],
                textposition='auto'
            ), row=3, col=2
        )
    
    # 9. Heatmap d'importance des features
    if ml_model and ml_model.feature_importance and 'random_forest' in ml_model.feature_importance:
        importance = ml_model.feature_importance['random_forest']
        top_features = list(importance.keys())[:8]
        importance_values = [[importance[f]] for f in top_features]
        
        fig.add_trace(
            go.Heatmap(
                z=importance_values,
                y=[f[:15] for f in top_features],
                x=['Importance'],
                colorscale='Plasma',
                showscale=True,
                colorbar=dict(title="Importance", x=1.0)
            ), row=3, col=3
        )
    
    fig.update_layout(
        height=1000,
        showlegend=True,
        title_text="üß† Analyse Deep Learning & ML Avanc√©e",
        title_x=0.5,
        title_font_size=24,
        font=dict(size=10)
    )
    
    return fig

def generate_sample_data(data_type="plat"):
    """G√©n√©ration de donn√©es d'exemple inchang√©e"""
    if data_type == "plat":
        return pd.DataFrame({
            'Nom': ['Thunder Bolt', 'Lightning Star', 'Storm King', 'Rain Dance', 'Wind Walker', 'Fire Dancer', 'Ocean Wave'],
            'Num√©ro de corde': ['1', '2', '3', '4', '5', '6', '7'],
            'Cote': ['3.2', '4.8', '7.5', '6.2', '9.1', '12.5', '15.0'],
            'Poids': ['56.5', '57.0', '58.5', '59.0', '57.5', '60.0', '61.5'],
            'Musique': ['1a2a3a1a', '2a1a4a3a', '3a3a1a2a', '1a4a2a1a', '4a2a5a3a', '5a3a6a4a', '6a5a7a8a'],
            '√Çge/Sexe': ['4H', '5M', '3F', '6H', '4M', '5H', '4F']
        })
    elif data_type == "attele":
        return pd.DataFrame({
            'Nom': ['Rapide √âclair', 'Foudre Noire', 'Vent du Nord', 'Temp√™te Rouge', 'Orage Bleu', 'Cyclone Vert'],
            'Num√©ro de corde': ['1', '2', '3', '4', '5', '6'],
            'Cote': ['4.2', '8.5', '15.0', '3.8', '6.8', '10.2'],
            'Poids': ['68.0', '68.0', '68.0', '68.0', '68.0', '68.0'],
            'Musique': ['2a1a4a1a', '4a3a2a5a', '6a5a8a7a', '1a2a1a3a', '3a4a5a2a', '5a6a4a8a'],
            '√Çge/Sexe': ['5H', '6M', '4F', '7H', '5M', '6H']
        })
    else:  # premium/obstacle
        return pd.DataFrame({
            'Nom': ['Ace Impact', 'Torquator Tasso', 'Adayar', 'Tarnawa', 'Chrono Genesis', 'Mishriff', 'Love'],
            'Num√©ro de corde': ['1', '2', '3', '4', '5', '6', '7'],
            'Cote': ['3.2', '4.8', '7.5', '6.2', '9.1', '5.5', '11.0'],
            'Poids': ['70.5', '71.0', '69.5', '68.5', '69.0', '70.0', '68.0'],
            'Musique': ['1a1a2a1a', '1a3a1a2a', '2a1a4a1a', '1a2a1a3a', '3a1a2a1a', '1a1a1a2a', '2a3a1a4a'],
            '√Çge/Sexe': ['6H', '7H', '5H', '6F', '7F', '6H', '5F']
        })

def main():
    st.markdown('<h1 class="main-header">üß† Analyseur Hippique Deep Learning Pro</h1>', unsafe_allow_html=True)
    st.markdown("*Pr√©dictions avanc√©es avec r√©seaux de neurones, optimisation bay√©sienne et ensemble learning*")
    
    # Sidebar avec configuration avanc√©e
    with st.sidebar:
        st.header("üõ†Ô∏è Configuration ML Avanc√©e")
        
        # Param√®tres de base
        race_type = st.selectbox("üèÅ Type de course", 
                                ["AUTO", "PLAT", "ATTELE_AUTOSTART", "ATTELE_VOLTE", "OBSTACLE"])
        
        use_neural = st.checkbox("üß† Activer Deep Learning", value=True)
        neural_confidence = st.slider("üéØ Poids Neural Network", 0.1, 0.9, 0.4, 0.05)
        
        st.subheader("ü§ñ Architecture IA")
        if use_neural:
            st.success("‚úÖ R√©seau de Neurones Profond")
            st.info("‚Ä¢ 4 couches cach√©es adaptatives")
            st.info("‚Ä¢ Batch normalization")
            st.info("‚Ä¢ Dropout & r√©gularisation L2")
            st.info("‚Ä¢ Optimisation Adam")
        
        st.success("‚úÖ Ensemble Learning")
        st.info("‚Ä¢ Random Forest (300 arbres)")
        st.info("‚Ä¢ Gradient Boosting")
        st.info("‚Ä¢ R√©gression Bay√©sienne")
        st.info("‚Ä¢ Optimisation automatique")
        
        st.subheader("üìä Features Engineering")
        st.success("**60+ features** sophistiqu√©es")
        st.info("‚Ä¢ Transformations math√©matiques")
        st.info("‚Ä¢ Interactions complexes")
        st.info("‚Ä¢ Clustering automatique")
        st.info("‚Ä¢ S√©lection par corr√©lation")
        
        # M√©triques de validation
        st.subheader("üî¨ Validation")
        st.info("‚Ä¢ Cross-validation temporelle")
        st.info("‚Ä¢ Optimisation bay√©sienne")
        st.info("‚Ä¢ Estimation d'incertitude")
        st.info("‚Ä¢ Corr√©lations multiples")
    
    # Interface principale avec tabs
    tab1, tab2, tab3, tab4 = st.tabs(["üåê URL Geny.fr", "üìÅ Upload CSV", "üß™ Test Data", "üìä Analyse Avanc√©e"])
    
    df_final = None
    
    with tab1:
        st.subheader("üîç Analyse d'URL de Course Geny.fr")
        st.markdown("*Scraping automatique des donn√©es de course en temps r√©el*")
        
        col1, col2 = st.columns([4, 1])
        with col1:
            url = st.text_input(
                "üåê URL de la course:", 
                placeholder="https://www.geny.com/courses/...",
                help="Coller l'URL d'une course depuis Geny.fr"
            )
        with col2:
            st.markdown("<br>", unsafe_allow_html=True)
            analyze_button = st.button("üîç Analyser", type="primary")
        
        if analyze_button and url:
            with st.spinner("üîÑ Extraction des donn√©es Geny.fr..."):
                df, message = scrape_race_data(url)
                if df is not None:
                    st.success(f"‚úÖ {len(df)} chevaux extraits avec succ√®s depuis Geny.fr")
                    
                    # Aper√ßu des donn√©es
                    st.markdown("**üìã Donn√©es extraites:**")
                    st.dataframe(df, use_container_width=True, height=200)
                    
                    # Statistiques rapides
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        avg_odds = df['Cote'].apply(lambda x: safe_convert(x, float, 0)).mean()
                        st.metric("üìä Cote moyenne", f"{avg_odds:.1f}")
                    with col2:
                        fav_odds = df['Cote'].apply(lambda x: safe_convert(x, float, 999)).min()
                        st.metric("‚≠ê Favori", f"{fav_odds:.1f}")
                    with col3:
                        st.metric("üèá Partants", len(df))
                    
                    df_final = df
                else:
                    st.error(f"‚ùå {message}")
                    st.info("üí° V√©rifiez que l'URL est correcte et accessible")
    
    with tab2:
        st.subheader("üì§ Upload de fichier CSV")
        st.markdown("*Format attendu: Nom, Num√©ro de corde, Cote, Poids, Musique, √Çge/Sexe*")
        
        uploaded_file = st.file_uploader("Choisir un fichier CSV", type="csv")
        if uploaded_file:
            try:
                df_final = pd.read_csv(uploaded_file)
                st.success(f"‚úÖ {len(df_final)} chevaux charg√©s depuis le fichier CSV")
                st.dataframe(df_final.head(), use_container_width=True)
            except Exception as e:
                st.error(f"‚ùå Erreur de lecture: {e}")
    
    with tab3:
        st.subheader("üß™ Donn√©es de Test pour Deep Learning")
        st.markdown("*Jeux de donn√©es optimis√©s pour tester les algorithmes ML*")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            if st.button("üèÉ Test Plat", use_container_width=True):
                df_final = generate_sample_data("plat")
                st.success("‚úÖ Course de PLAT charg√©e")
                st.info("7 chevaux avec handicap poids")
                
        with col2:
            if st.button("üöó Test Attel√©", use_container_width=True):
                df_final = generate_sample_data("attele")
                st.success("‚úÖ Course d'ATTEL√â charg√©e")
                st.info("6 chevaux poids uniformes")
                
        with col3:
            if st.button("üöß Test Obstacle", use_container_width=True):
                df_final = generate_sample_data("premium")
                st.success("‚úÖ Course d'OBSTACLES charg√©e")
                st.info("7 chevaux exp√©riment√©s")
                
        with col4:
            if st.button("üé≤ Test Mixte", use_container_width=True):
                # G√©n√©ration al√©atoire
                np.random.seed(int(datetime.now().timestamp()) % 1000)
                df_final = generate_sample_data(np.random.choice(["plat", "attele", "premium"]))
                st.success("‚úÖ Course AL√âATOIRE charg√©e")
                st.info("Dataset surpris g√©n√©r√©")
        
        if df_final is not None:
            st.markdown("**üìã Donn√©es charg√©es:**")
            st.dataframe(df_final, use_container_width=True)
    
    with tab4:
        st.subheader("üìä Analyse Statistique Avanc√©e")
        st.markdown("*Visualisations et m√©triques pour les experts en data science*")
        
        if df_final is not None:
            df_prep = prepare_data(df_final)
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("**üìà Distribution des Cotes**")
                fig_odds = px.histogram(
                    df_prep, x='odds_numeric', 
                    title="R√©partition des Cotes",
                    labels={'odds_numeric': 'Cotes', 'count': 'Fr√©quence'}
                )
                st.plotly_chart(fig_odds, use_container_width=True)
                
            with col2:
                st.markdown("**‚öñÔ∏è Analyse des Poids**")
                fig_weight = px.box(
                    df_prep, y='weight_kg',
                    title="Distribution des Poids",
                    labels={'weight_kg': 'Poids (kg)'}
                )
                st.plotly_chart(fig_weight, use_container_width=True)
            
            # Corr√©lations
            numeric_cols = ['odds_numeric', 'draw_numeric', 'weight_kg']
            if len(numeric_cols) >= 2:
                corr_matrix = df_prep[numeric_cols].corr()
                fig_corr = px.imshow(
                    corr_matrix, 
                    title="Matrice de Corr√©lations",
                    color_continuous_scale='RdBu'
                )
                st.plotly_chart(fig_corr, use_container_width=True)
        else:
            st.info("üîç Chargez des donn√©es pour voir l'analyse statistique")
    
    # === ANALYSE PRINCIPALE AVEC DEEP LEARNING ===
    if df_final is not None and len(df_final) > 0:
        st.markdown("---")
        st.markdown('<div class="neural-network"><h2>üß† Analyse Deep Learning & Pr√©dictions IA</h2></div>', 
                   unsafe_allow_html=True)
        
        df_prepared = prepare_data(df_final)
        if len(df_prepared) == 0:
            st.error("‚ùå Aucune donn√©e valide apr√®s pr√©paration")
            return
        
        # D√©tection automatique du type de course
        if race_type == "AUTO":
            detected_type = auto_detect_race_type(df_prepared)
        else:
            detected_type = race_type
            config = RACE_CONFIGS[detected_type]
            st.info(f"üìã **Type s√©lectionn√©**: {config['description']}")
        
        # === MACHINE LEARNING AVANC√â ===
        ml_model = AdvancedNeuralHorseRacingML()
        
        with st.spinner("üöÄ Entra√Ænement du Deep Learning et des mod√®les avanc√©s..."):
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            try:
                # √âtape 1: Cr√©ation des features
                status_text.text("üîß Ing√©nierie des features avanc√©es...")
                progress_bar.progress(20)
                
                X_advanced = ml_model.create_ultra_advanced_features(df_prepared, detected_type)
                st.success(f"‚úÖ **{len(X_advanced.columns)} features** cr√©√©es avec succ√®s")
                
                # √âtape 2: Target synth√©tique am√©lior√©
                status_text.text("üéØ G√©n√©ration du target optimal...")
                progress_bar.progress(40)
                
                # Target plus sophistiqu√© bas√© sur plusieurs facteurs
                y_synthetic = (
                    X_advanced['odds_inv'] * 0.30 +
                    X_advanced['music_class_rating'] * 0.25 +
                    X_advanced['composite_score'] * 0.20 +
                    X_advanced['position_advantage'] * 0.15 +
                    X_advanced['age_factor'] * 0.10 +
                    np.random.normal(0, 0.03, len(X_advanced))  # Bruit r√©duit
                )
                
                # √âtape 3: Entra√Ænement ML
                status_text.text("üß† Entra√Ænement des r√©seaux de neurones...")
                progress_bar.progress(60)
                
                predictions, cv_results, confidence_scores = ml_model.train_advanced_ensemble(
                    X_advanced, y_synthetic, detected_type
                )
                
                progress_bar.progress(80)
                
                # √âtape 4: Finalisation
                df_prepared['neural_score'] = predictions
                df_prepared['confidence'] = confidence_scores
                df_prepared['score_final'] = predictions
                
                status_text.text("‚úÖ Mod√®les entra√Æn√©s avec succ√®s!")
                progress_bar.progress(100)
                
                # Affichage des m√©triques ML
                st.markdown("### üìä M√©triques de Performance ML")
                
                cols = st.columns(len(cv_results) if cv_results else 4)
                for i, (model, scores) in enumerate(cv_results.items()):
                    if i < len(cols):
                        with cols[i]:
                            mse_score = scores['mean']
                            r2_equivalent = max(0, 1 - mse_score)  # Approximation R¬≤
                            
                            if model == 'neural_network':
                                st.metric(
                                    "üß† Neural Net", 
                                    f"{r2_equivalent:.3f}",
                                    help="Performance du r√©seau de neurones"
                                )
                            elif model == 'bayesian':
                                st.metric(
                                    "üìä Bay√©sien", 
                                    f"{r2_equivalent:.3f}",
                                    help="Mod√®le avec estimation d'incertitude"
                                )
                            else:
                                st.metric(
                                    f"ü§ñ {model.replace('_', ' ').title()}", 
                                    f"{r2_equivalent:.3f}"
                                )
                
                # M√©triques globales
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    avg_conf = confidence_scores.mean()
                    st.metric("üéØ Confiance Moy.", f"{avg_conf:.1%}")
                with col2:
                    if ml_model.model_weights:
                        neural_weight = ml_model.model_weights.get('neural_network', 0)
                        st.metric("üß† Poids Neural", f"{neural_weight:.1%}")
                with col3:
                    feature_count = len(X_advanced.columns)
                    st.metric("üîß Features", f"{feature_count}")
                with col4:
                    uncertainty_mean = ml_model.uncertainty_estimates.get('mean_uncertainty', 0)
                    st.metric("üìä Incertitude", f"{uncertainty_mean:.3f}")
                
            except Exception as e:
                st.error(f"‚ö†Ô∏è Erreur lors de l'entra√Ænement ML: {e}")
                # Fallback vers score simple
                df_prepared['score_final'] = 1 / (df_prepared['odds_numeric'] + 0.1)
                df_prepared['confidence'] = 0.5
                predictions = df_prepared['score_final'].values
                cv_results = {}
                confidence_scores = np.ones(len(df_prepared)) * 0.5
        
        # === CLASSEMENT FINAL ===
        df_ranked = df_prepared.sort_values('score_final', ascending=False).reset_index(drop=True)
        df_ranked['rang'] = range(1, len(df_ranked) + 1)
        
        # === AFFICHAGE DES R√âSULTATS ===
        st.markdown("---")
        st.subheader("üèÜ Classement IA avec Scores de Confiance")
        
        col_results, col_insights = st.columns([3, 2])
        
        with col_results:
            # Tableau des r√©sultats avec styling
            display_cols = ['rang', 'Nom', 'Cote', 'Num√©ro de corde']
            if 'Poids' in df_ranked.columns:
                display_cols.append('Poids')
            display_cols.extend(['score_final', 'confidence'])
            
            display_df = df_ranked[display_cols].copy()
            display_df['Score IA'] = display_df['score_final'].apply(lambda x: f"{x:.3f}")
            display_df['Confiance'] = display_df['confidence'].apply(lambda x: f"{x:.1%}")
            display_df = display_df.drop(['score_final', 'confidence'], axis=1)
            
            # Coloration par rang
            def color_rank(val):
                if val == 1:
                    return 'background-color: gold; font-weight: bold'
                elif val == 2:
                    return 'background-color: silver; font-weight: bold'
                elif val == 3:
                    return 'background-color: #CD7F32; font-weight: bold'
                else:
                    return ''
            
            styled_df = display_df.style.applymap(color_rank, subset=['rang'])
            st.dataframe(styled_df, use_container_width=True, height=400)
        
        with col_insights:
            st.markdown("#### üéØ Insights IA")
            
            # Top 3 avec analyse
            for i in range(min(3, len(df_ranked))):
                horse = df_ranked.iloc[i]
                conf = horse['confidence']
                score = horse['score_final']
                odds = horse['odds_numeric']
                
                # Calcul de la valeur
                expected_prob = 1 / odds
                ai_prob = score
                value = ai_prob - expected_prob
                
                if conf >= 0.7:
                    conf_emoji = "üü¢"
                    conf_text = "Haute"
                elif conf >= 0.5:
                    conf_emoji = "üü°" 
                    conf_text = "Moyenne"
                else:
                    conf_emoji = "üî¥"
                    conf_text = "Faible"
                
                if value > 0.1:
                    value_emoji = "üíé"
                    value_text = "Excellent"
                elif value > 0:
                    value_emoji = "üëç"
                    value_text = "Bon"
                else:
                    value_emoji = "‚ö†Ô∏è"
                    value_text = "Risqu√©"
                
                st.markdown(f"""
                <div class="prediction-box">
                    <strong>{i+1}. {horse['Nom']}</strong><br>
                    üéØ Score IA: <strong>{score:.3f}</strong><br>
                    üìä Cote: <strong>{odds:.1f}</strong><br>
                    {conf_emoji} Confiance: <strong>{conf_text} ({conf:.1%})</strong><br>
                    {value_emoji} Valeur: <strong>{value_text}</strong>
                </div>
                """, unsafe_allow_html=True)
            
            # Recommandations strat√©giques
            st.markdown("#### üí° Recommandations")
            
            # Meilleure valeur
            df_ranked['value_score'] = df_ranked['score_final'] - (1 / df_ranked['odds_numeric'])
            best_value = df_ranked.loc[df_ranked['value_score'].idxmax()]
            
            st.success(f"üíé **Meilleure valeur**: {best_value['Nom']} (Rang {best_value['rang']})")
            
            # Alerte sur les favoris
            weak_fav = df_ranked[(df_ranked['odds_numeric'] < 5) & (df_ranked['rang'] > 3)]
            if len(weak_fav) > 0:
                st.warning(f"‚ö†Ô∏è {len(weak_fav)} favori(s) mal class√©(s)")
            
            # Outsider prometteur
            surprise = df_ranked[(df_ranked['odds_numeric'] > 10) & (df_ranked['rang'] <= 3)]
            if len(surprise) > 0:
                st.info(f"üé≤ {len(surprise)} outsider(s) dans le Top 3")
        
        # === VISUALISATIONS ULTRA-AVANC√âES ===
        st.markdown("---")
        st.subheader("üìä Visualisations Deep Learning")
        
        fig_advanced = create_advanced_neural_visualization(df_ranked, ml_model)
        st.plotly_chart(fig_advanced, use_container_width=True)
        
        # === ANALYSE D√âTAILL√âE DES FEATURES ===
        if ml_model.feature_correlations:
            st.markdown("---")
            st.subheader("üî¨ Analyse D√©taill√©e des Features")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("**üéØ Top 15 Features par Corr√©lation**")
                
                # Tri des features par score composite
                sorted_features = sorted(
                    ml_model.feature_correlations.items(),
                    key=lambda x: x[1]['composite_score'],
                    reverse=True
                )[:15]
                
                feature_analysis_df = pd.DataFrame([
                    {
                        'Feature': feat[0],
                        'Corr√©lation': f"{feat[1]['composite_score']:.3f}",
                        'Pearson': f"{feat[1]['pearson']:.3f}",
                        'Spearman': f"{feat[1]['spearman']:.3f}",
                        'Info Mutuelle': f"{feat[1]['mutual_info']:.3f}"
                    }
                    for feat in sorted_features
                ])
                
                st.dataframe(feature_analysis_df, use_container_width=True, height=400)
            
            with col2:
                st.markdown("**üèÜ Importance par Mod√®le**")
                
                if 'random_forest' in ml_model.feature_importance:
                    rf_importance = ml_model.feature_importance['random_forest']
                    
                    importance_df = pd.DataFrame([
                        {'Feature': k[:20], 'Importance RF': f"{v:.4f}"}
                        for k, v in list(rf_importance.items())[:15]
                    ])
                    
                    st.dataframe(importance_df, use_container_width=True, height=200)
                
                if ml_model.model_weights:
                    st.markdown("**‚öñÔ∏è Poids des Mod√®les Optimis√©s**")
                    weights_df = pd.DataFrame([
                        {'Mod√®le': k.replace('_', ' ').title(), 'Poids': f"{v:.3f}"}
                        for k, v in ml_model.model_weights.items()
                    ])
                    st.dataframe(weights_df, use_container_width=True, height=200)
        
        # === EXPORT ET RAPPORTS ===
        st.markdown("---")
        st.subheader("üíæ Export et Rapports Avanc√©s")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            csv_data = df_ranked.to_csv(index=False)
            st.download_button(
                "üìÑ CSV Complet",
                csv_data,
                f"pronostic_dl_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                "text/csv",
                use_container_width=True
            )
        
        with col2:
            # Export JSON avec m√©tadonn√©es ML
            export_data = {
                'metadata': {
                    'timestamp': datetime.now().isoformat(),
                    'race_type': detected_type,
                    'model_weights': ml_model.model_weights,
                    'avg_confidence': float(confidence_scores.mean()),
                    'features_count': len(X_advanced.columns) if 'X_advanced' in locals() else 0
                },
                'predictions': df_ranked.to_dict('records')
            }
            
            json_data = json.dumps(export_data, indent=2, default=str)
            st.download_button(
                "üìä JSON + ML",
                json_data,
                f"pronostic_ml_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                "application/json",
                use_container_width=True
            )
        
        with col3:
            # Rapport d'analyse technique
            technical_report = f"""
RAPPORT TECHNIQUE - DEEP LEARNING HIPPIQUE
{'='*60}
Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Type de course: {detected_type}
Nombre de chevaux: {len(df_ranked)}

ARCHITECTURE ML:
{'-'*60}
‚Ä¢ R√©seau de neurones: {len(X_advanced.columns) if 'X_advanced' in locals() else 'N/A'} ‚Üí 128 ‚Üí 64 ‚Üí 32 ‚Üí 16 ‚Üí 1
‚Ä¢ Features engineering: {len(X_advanced.columns) if 'X_advanced' in locals() else 'N/A'} features cr√©√©es
‚Ä¢ Validation: Cross-validation temporelle
‚Ä¢ Optimisation: Poids automatiques

PERFORMANCE MOD√àLES:
{'-'*60}
"""
            
            if cv_results:
                for model, scores in cv_results.items():
                    r2_equiv = max(0, 1 - scores['mean'])
                    technical_report += f"‚Ä¢ {model}: R¬≤ ‚âà {r2_equiv:.3f} (¬±{scores['std']:.3f})\n"
            
            technical_report += f"""
POIDS OPTIMIS√âS:
{'-'*60}
"""
            if ml_model.model_weights:
                for model, weight in ml_model.model_weights.items():
                    technical_report += f"‚Ä¢ {model}: {weight:.3f}\n"
            
            technical_report += f"""

TOP 5 PR√âDICTIONS:
{'-'*60}
"""
            for i in range(min(5, len(df_ranked))):
                horse = df_ranked.iloc[i]
                technical_report += f"{i+1}. {horse['Nom']} - Score: {horse['score_final']:.3f} - Confiance: {horse['confidence']:.1%}\n"
            
            st.download_button(
                "üî¨ Rapport Tech",
                technical_report,
                f"rapport_technique_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
                "text/plain",
                use_container_width=True
            )
        
        with col4:
            # Combinaisons optimales
            if len(df_ranked) >= 5:
                combinations = {
                    'Quint√©': df_ranked.head(5)['Nom'].tolist(),
                    'Trio': df_ranked.head(3)['Nom'].tolist(),
                    'Coupl√©': df_ranked.head(2)['Nom'].tolist()
                }
                
                combinations_text = f"""
COMBINAISONS IA OPTIMALES
{'='*40}
G√©n√©r√©es le: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

üèÜ QUINT√â+ (ordre):
{'-'*40}
"""
                for i, horse in enumerate(combinations['Quint√©'], 1):
                    combinations_text += f"{i}. {horse}\n"
                
                combinations_text += f"""
ü•á TRIO (ordre):
{'-'*40}
"""
                for i, horse in enumerate(combinations['Trio'], 1):
                    combinations_text += f"{i}. {horse}\n"
                
                combinations_text += f"""
üíØ COUPL√â:
{'-'*40}
{combinations['Coupl√©'][0]} - {combinations['Coupl√©'][1]}

‚ö° CONFIANCE MOYENNE: {confidence_scores.mean():.1%}
üß† TYPE ML: Deep Learning + Ensemble
"""
                
                st.download_button(
                    "üéØ Combinaisons",
                    combinations_text,
                    f"combinaisons_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
                    "text/plain",
                    use_container_width=True
                )
    
    else:
        # Message d'accueil avec info sur les am√©liorations
        st.markdown("---")
        st.markdown("""
        ## üöÄ Am√©liorations Deep Learning Int√©gr√©es
        
        **üß† Architecture Neuronale Avanc√©e:**
        - R√©seau √† 4 couches avec batch normalization
        - Dropout adaptatif et r√©gularisation L2
        - Architecture adaptative selon le type de course
        - Optimiseur Adam avec scheduler automatique
        
        **üîß Features Engineering Ultra-Pouss√©e:**
        - 60+ features sophistiqu√©es cr√©√©es automatiquement
        - Transformations math√©matiques des cotes (log, sqrt, power)
        - Interactions complexes entre variables
        - Clustering automatique des performances
        - Analyse temporelle de la musique (forme r√©cente pond√©r√©e)
        
        **üìä Ensemble Learning Optimis√©:**
        - Combinaison de Random Forest, Gradient Boosting, Ridge, ElasticNet
        - Mod√®le bay√©sien pour estimation d'incertitude
        - Optimisation automatique des poids par minimisation MSE
        - Validation crois√©e temporelle
        
        **üéØ Corr√©lations et S√©lection:**
        - Calcul de corr√©lations Pearson, Spearman et information mutuelle
        - S√©lection automatique des meilleures features
        - Analyse de l'importance par mod√®le
        - PCA automatique si n√©cessaire
        
        **üìà M√©triques et Confiance:**
        - Score de confiance bas√© sur la variance inter-mod√®les
        - Estimation d'incertitude bay√©sienne
        - Qualit√© des features int√©gr√©e
        - Performance de validation crois√©e
        
        **‚öôÔ∏è Adaptations par Type de Course:**
        - Param√®tres sp√©cialis√©s pour PLAT, ATTEL√â, OBSTACLES
        - Poids adaptatifs selon l'importance (poids, position, √¢ge)
        - Biais de piste configurables
        - √Çges optimaux par discipline
        
        **üé® Visualisations Avanc√©es:**
        - 9 graphiques sp√©cialis√©s pour l'analyse ML
        - Heatmaps d'importance et de corr√©lations
        - Historique d'entra√Ænement du r√©seau de neurones
        - Clustering et incertitude bay√©sienne
        
        üëÜ **Chargez des donn√©es pour voir le nouveau syst√®me en action !**
        """)

if __name__ == "__main__":
    main()
Response
Created file /home/user/pronostic_hippique_advanced_ml.py (71007 characters)
